{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
        "import numpy as np\n",
        "import wandb\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "# import cv2\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameters Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Paths ===\n",
        "HR_ROOT = \"HR\"\n",
        "LR_ROOT = \"LR\"\n",
        "SAVE_DIR = \"saved_models\"\n",
        "\n",
        "# === Model Architecture ===\n",
        "NUM_RES_BLOCKS = 10\n",
        "IN_CHANNELS = 9  # 3 frames × 3 channels\n",
        "OUT_CHANNELS = 3\n",
        "FEATURES = 128\n",
        "UPSCALE_FACTOR = 4  # 120×214 → 480×854 (~4x)\n",
        "\n",
        "# === Training ===\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 100\n",
        "LR = 1e-3\n",
        "SAVE_FREQ = 2  # Save every 2 epochs\n",
        "USE_PERCEPTUAL = False # Change in hyperparameters if needed\n",
        "\n",
        "# === Data Management ===\n",
        "TEST_RATIO = 0.1\n",
        "VAL_RATIO = 0.1\n",
        "DATA_SPLIT_SEED = 42\n",
        "\n",
        "# === WandB ===\n",
        "WANDB_PROJECT = \"video-superres\"\n",
        "WANDB_ENTITY = \"\"  # Your WandB username/team\n",
        "# === WandB Logging ===\n",
        "LOG_FREQ = 50  # Log every 50 batches\n",
        "LOG_IMG_FREQ = 100  # Log images every 100 batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### WandB Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "wandb.init(\n",
        "    project=WANDB_PROJECT,\n",
        "    entity=WANDB_ENTITY,\n",
        "    config={\n",
        "        \"learning_rate\": LR,\n",
        "        \"architecture\": \"MultiFrame-CNN\",\n",
        "        \"dataset\": \"CustomVideoFrames\",\n",
        "        \"epochs\": NUM_EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"upscale_factor\": UPSCALE_FACTOR\n",
        "    }\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Class Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VideoSRDataset(Dataset):\n",
        "    def __init__(self, hr_dir, lr_dir, seq_ids, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hr_dir: Path to HR frames root\n",
        "            lr_dir: Path to LR frames root\n",
        "            seq_ids: List of sequence identifiers (format: \"video_XXXX_seq_X\")\n",
        "            transform: Optional transforms\n",
        "        \"\"\"\n",
        "        self.hr_dir = hr_dir\n",
        "        self.lr_dir = lr_dir\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        \n",
        "        # Build samples list\n",
        "        for seq_id in seq_ids:\n",
        "            self._add_sequence_samples(seq_id)\n",
        "\n",
        "    def _add_sequence_samples(self, seq_id):\n",
        "        \"\"\"Add all valid frame triplets from a sequence\"\"\"\n",
        "        seq_lr_path = os.path.join(self.lr_dir, seq_id)\n",
        "        seq_hr_path = os.path.join(self.hr_dir, seq_id)\n",
        "        \n",
        "        # Get sorted frame paths\n",
        "        lr_frames = sorted([f for f in os.listdir(seq_lr_path) if f.endswith(\".png\")])\n",
        "        hr_frames = sorted([f for f in os.listdir(seq_hr_path) if f.endswith(\".png\")])\n",
        "\n",
        "        assert len(lr_frames) >= 5, f\"{seq_id} has only {len(lr_frames)} LR frames!\"\n",
        "        assert len(hr_frames) >= 5, f\"{seq_id} has only {len(hr_frames)} HR frames!\"\n",
        "        \n",
        "        # Each sequence contains 5 frames: use middle 3 as anchor points\n",
        "        for i in range(1, 4):  # Center frames 1,2,3 (0-based)\n",
        "            lr_triplet = [\n",
        "                os.path.join(seq_lr_path, lr_frames[i-1]),\n",
        "                os.path.join(seq_lr_path, lr_frames[i]),\n",
        "                os.path.join(seq_lr_path, lr_frames[i+1])\n",
        "            ]\n",
        "            hr_target = os.path.join(seq_hr_path, hr_frames[i])\n",
        "            \n",
        "            self.samples.append((lr_triplet, hr_target))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            lr_paths, hr_path = self.samples[idx]\n",
        "            \n",
        "            # Load LR frames with progress indication\n",
        "            lr_stack = []\n",
        "            for i, path in enumerate(lr_paths):\n",
        "                if not os.path.exists(path):\n",
        "                    raise FileNotFoundError(f\"LR frame {i} missing: {path}\")\n",
        "                img = Image.open(path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                lr_stack.append(img)\n",
        "                \n",
        "            # Load HR frame\n",
        "            if not os.path.exists(hr_path):\n",
        "                raise FileNotFoundError(f\"HR frame missing: {hr_path}\")\n",
        "            hr_img = Image.open(hr_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                hr_img = self.transform(hr_img)\n",
        "                \n",
        "            return torch.cat(lr_stack, dim=0), hr_img\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\nError loading sample {idx}:\")\n",
        "            print(f\"Sequence: {self.samples[idx][0][0].split('/')[-2]}\")\n",
        "            print(f\"LR paths: {lr_paths}\")\n",
        "            print(f\"HR path: {hr_path}\")\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Splitting & Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First get all video sequence identifiers\n",
        "all_sequences = os.listdir(HR_ROOT)\n",
        "\n",
        "print(\"Total sequences in HR_ROOT:\", len(all_sequences))\n",
        "assert len(all_sequences) > 0, \"No sequences found in HR_ROOT!\"\n",
        "\n",
        "# Extract unique video IDs (e.g., \"video_0000\" from \"video_0000_seq_0\")\n",
        "video_ids = list(set([\"_\".join(s.split(\"_\")[:2]) for s in all_sequences]))\n",
        "video_ids.sort()\n",
        "\n",
        "# Split video IDs into train/val/test\n",
        "random.seed(DATA_SPLIT_SEED)\n",
        "random.shuffle(video_ids)\n",
        "\n",
        "num_total = len(video_ids)\n",
        "num_test = int(num_total * TEST_RATIO)\n",
        "num_val = int(num_total * VAL_RATIO)\n",
        "num_train = num_total - num_test - num_val\n",
        "\n",
        "train_vids = video_ids[:num_train]\n",
        "val_vids = video_ids[num_train:num_train+num_val]\n",
        "test_vids = video_ids[num_train+num_val:]\n",
        "\n",
        "# Now collect all sequences for each split\n",
        "def get_split_sequences(video_list):\n",
        "    sequences = []\n",
        "    for vid in video_list:\n",
        "        # Get all sequences for this video\n",
        "        seqs = [s for s in all_sequences if s.startswith(vid)]\n",
        "        sequences.extend(seqs)\n",
        "    return sequences\n",
        "\n",
        "train_seqs = get_split_sequences(train_vids)\n",
        "val_seqs = get_split_sequences(val_vids)\n",
        "test_seqs = get_split_sequences(test_vids)\n",
        "\n",
        "print(f\"Total sequences - Train: {len(train_seqs)}, Val: {len(val_seqs)}, Test: {len(test_seqs)}\")\n",
        "\n",
        "# After splitting\n",
        "print(\"\\nSplit Summary:\")\n",
        "print(f\"Train Videos: {len(train_vids)} | Val Videos: {len(val_vids)} | Test Videos: {len(test_vids)}\")\n",
        "print(f\"Train Seqs: {len(train_seqs)} | Val Seqs: {len(val_seqs)} | Test Seqs: {len(test_seqs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DataLoader Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic transforms (normalize to [-1, 1] range)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = VideoSRDataset(HR_ROOT, LR_ROOT, train_seqs, transform=transform)\n",
        "val_dataset = VideoSRDataset(HR_ROOT, LR_ROOT, val_seqs, transform=transform)\n",
        "test_dataset = VideoSRDataset(HR_ROOT, LR_ROOT, test_seqs, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=8,\n",
        "    prefetch_factor=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=8,\n",
        "    prefetch_factor=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Quick sanity check\n",
        "sample_lr, sample_hr = next(iter(train_loader))\n",
        "print(f\"LR input shape: {sample_lr.shape}\")  # Should be [B, 9, H, W]\n",
        "print(f\"HR target shape: {sample_hr.shape}\") # Should be [B, 3, 4H, 4W]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### residual block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.conv2(x)\n",
        "        return x + residual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Base SR Model (first stage)\n",
        "class SRModel(nn.Module):\n",
        "    def __init__(self, in_channels=9, out_channels=3, features=64, num_res_blocks=5, upscale_factor=4):\n",
        "        super().__init__()\n",
        "        # Initial feature extraction\n",
        "        self.conv1 = nn.Conv2d(in_channels, features, kernel_size=3, padding=1)\n",
        "        # Residual blocks\n",
        "        self.res_blocks = nn.Sequential(*[ResidualBlock(features) for _ in range(num_res_blocks)])\n",
        "        # Upscaling\n",
        "        self.conv2 = nn.Conv2d(features, out_channels * (upscale_factor ** 2), kernel_size=3, padding=1)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.res_blocks(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pixel_shuffle(x)\n",
        "        return x\n",
        "\n",
        "# New refinement network (second stage) to boost high-frequency details.\n",
        "class RefinementNet(nn.Module):\n",
        "    def __init__(self, channels=3, features=64, num_res_blocks=3):\n",
        "        super(RefinementNet, self).__init__()\n",
        "        self.conv_in = nn.Conv2d(channels, features, kernel_size=3, padding=1)\n",
        "        # A few residual blocks for texture refinement\n",
        "        res_blocks = [ResidualBlock(features) for _ in range(num_res_blocks)]\n",
        "        self.res_blocks = nn.Sequential(*res_blocks)\n",
        "        self.conv_out = nn.Conv2d(features, channels, kernel_size=3, padding=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv_in(x))\n",
        "        out = self.res_blocks(out)\n",
        "        out = self.conv_out(out)\n",
        "        # Adding a residual connection so that the refinement net refines rather than re-predicts entirely.\n",
        "        return x + out\n",
        "\n",
        "# Composite model that feeds the output of the base model into the refinement network.\n",
        "class TwoStageSRModel(nn.Module):\n",
        "    def __init__(self, base_model, refinement_model):\n",
        "        super(TwoStageSRModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.refinement_model = refinement_model\n",
        "        \n",
        "    def forward(self, x):\n",
        "        base_output = self.base_model(x)\n",
        "        refined_output = self.refinement_model(base_output)\n",
        "        return refined_output\n",
        "\n",
        "# Instantiate the base model\n",
        "base_model = SRModel(\n",
        "    in_channels=IN_CHANNELS,      # from your hyperparameters (9 channels)\n",
        "    out_channels=OUT_CHANNELS,    # (3 channels)\n",
        "    features=FEATURES,            # e.g., 64\n",
        "    num_res_blocks=NUM_RES_BLOCKS,  # e.g., 5\n",
        "    upscale_factor=UPSCALE_FACTOR   # e.g., 4\n",
        ").to(device)\n",
        "\n",
        "# Optionally, if you already have pretrained weights for the base model,\n",
        "# load them here. For example:\n",
        "# checkpoint = torch.load('saved_models/best.pth', map_location=device)\n",
        "# base_model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "# (Optional) Freeze base model parameters if you want to train only the refinement net:\n",
        "# for param in base_model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# Instantiate the refinement network\n",
        "refinement_net = RefinementNet(channels=OUT_CHANNELS, features=FEATURES, num_res_blocks=3).to(device)\n",
        "\n",
        "# Create the composite two-stage model\n",
        "model = TwoStageSRModel(base_model, refinement_net).to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(model)\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loss & Optimizer Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# L1 Loss (MAE)\n",
        "l1_loss = nn.L1Loss().to(device)\n",
        "\n",
        "# Optional Perceptual Loss (VGG-based)\n",
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = torchvision.models.vgg19(pretrained=True).features[:35].eval()\n",
        "        for param in vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.vgg = vgg.to(device)\n",
        "        self.loss = nn.L1Loss()\n",
        "        \n",
        "    def forward(self, pred, target):\n",
        "        vgg_pred = self.vgg(self.normalize_vgg(pred))\n",
        "        vgg_target = self.vgg(self.normalize_vgg(target.detach()))\n",
        "        return self.loss(vgg_pred, vgg_target)\n",
        "    \n",
        "    def normalize_vgg(self, x):\n",
        "        # Convert from [-1,1] range to VGG expected [0,1]\n",
        "        return (x + 1) / 2\n",
        "\n",
        "# Set USE_PERCEPTUAL = False to disable\n",
        "\n",
        "perceptual_weight = 0.2 if USE_PERCEPTUAL else 0.0\n",
        "\n",
        "perceptual_loss = VGGLoss().to(device) if USE_PERCEPTUAL else None\n",
        "\n",
        "# Combined loss\n",
        "def compute_loss(pred, target):\n",
        "    loss = l1_loss(pred, target)\n",
        "    if USE_PERCEPTUAL:\n",
        "        loss += perceptual_weight * perceptual_loss(pred, target)\n",
        "    return loss\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Metrics Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "psnr_metric = PeakSignalNoiseRatio().to(device)\n",
        "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
        "\n",
        "def compute_metrics(pred, target):\n",
        "    # Convert from [-1,1] to [0,1] range for metrics\n",
        "    pred_denorm = (pred + 1) / 2\n",
        "    target_denorm = (target + 1) / 2\n",
        "    \n",
        "    # PSNR\n",
        "    # psnr = PeakSignalNoiseRatio().to(device)\n",
        "    psnr_val = psnr_metric(pred_denorm, target_denorm)\n",
        "    \n",
        "    # SSIM\n",
        "    # ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
        "    ssim_val =ssim_metric(pred_denorm, target_denorm)\n",
        "    \n",
        "    return psnr_val.item(), ssim_val.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_predictions(model, epoch, num_samples=3):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        lr, hr = next(iter(val_loader))\n",
        "        lr, hr = lr.to(device), hr.to(device)\n",
        "        pred = model(lr)\n",
        "        \n",
        "        # Upsample LR to match HR dimensions\n",
        "        lr_upsampled = torch.nn.functional.interpolate(\n",
        "            lr[:, 3:6],  # Center frame\n",
        "            size=hr.shape[-2:],  # Target size\n",
        "            mode='bicubic',\n",
        "            align_corners=False\n",
        "        )\n",
        "        \n",
        "        # Denormalize\n",
        "        lr_vis = (lr_upsampled + 1) / 2\n",
        "        hr_vis = (hr + 1) / 2\n",
        "        pred_vis = (pred + 1) / 2\n",
        "        \n",
        "        comparisons = []\n",
        "        for i in range(num_samples):\n",
        "            comparison = torch.cat([\n",
        "                lr_vis[i].cpu(),\n",
        "                pred_vis[i].cpu(),\n",
        "                hr_vis[i].cpu()\n",
        "            ], dim=-1)  # Concatenate along width\n",
        "            comparisons.append(comparison)\n",
        "        \n",
        "        grid = torchvision.utils.make_grid(comparisons, nrow=1)\n",
        "        \n",
        "        wandb.log({\n",
        "            \"Predictions\": wandb.Image(grid, caption=f\"Epoch {epoch+1}: LR | Pred | HR\")\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Loop Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    psnr_total = 0.0\n",
        "    ssim_total = 0.0\n",
        "    \n",
        "    for batch_idx, (lr, hr) in enumerate(loader):\n",
        "        lr = lr.to(device)\n",
        "        hr = hr.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(lr)\n",
        "        outputs = outputs[:, :, :hr.size(2), :hr.size(3)]\n",
        "        loss = compute_loss(outputs, hr)\n",
        "        \n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Metrics\n",
        "        psnr, ssim = compute_metrics(outputs.detach(), hr.detach())\n",
        "\n",
        "\n",
        "         # Batch-level logging\n",
        "        if batch_idx % LOG_FREQ == 0:\n",
        "            wandb.log({\n",
        "                \"train_batch_loss\": loss.item(),\n",
        "                \"train_batch_psnr\": psnr,\n",
        "                \"train_batch_ssim\": ssim\n",
        "            })\n",
        "            \n",
        "        # Image logging\n",
        "        if batch_idx % LOG_IMG_FREQ == 0:\n",
        "            with torch.no_grad():\n",
        "                pred_vis = (outputs[:1].detach() + 1) / 2\n",
        "                lr_vis = (lr[:1, 3:6].detach() + 1) / 2\n",
        "                grid = torch.cat([lr_vis.cpu(), pred_vis.cpu()], dim=-1)\n",
        "                wandb.log({\n",
        "                    \"train_samples\": wandb.Image(grid, \n",
        "                    caption=f\"Batch {batch_idx}: LR | Pred\")\n",
        "                })\n",
        "\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        psnr_total += psnr\n",
        "        ssim_total += ssim\n",
        "        \n",
        "        # Progress update\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f\"  Batch {batch_idx}/{len(loader)} - Loss: {loss.item():.4f}\")\n",
        "            \n",
        "    avg_loss = epoch_loss / len(loader)\n",
        "    avg_psnr = psnr_total / len(loader)\n",
        "    avg_ssim = ssim_total / len(loader)\n",
        "    return avg_loss, avg_psnr, avg_ssim\n",
        "\n",
        "def validate(model, loader):\n",
        "    model.eval()\n",
        "    epoch_loss = 0.0\n",
        "    psnr_total = 0.0\n",
        "    ssim_total = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for lr, hr in loader:\n",
        "            lr = lr.to(device)\n",
        "            hr = hr.to(device)\n",
        "            \n",
        "            outputs = model(lr)\n",
        "            outputs = outputs[:, :, :hr.size(2), :hr.size(3)]\n",
        "            loss = compute_loss(outputs, hr)\n",
        "            \n",
        "            psnr, ssim = compute_metrics(outputs.detach(), hr.detach())\n",
        "            epoch_loss += loss.item()\n",
        "            psnr_total += psnr\n",
        "            ssim_total += ssim\n",
        "            \n",
        "    avg_loss = epoch_loss / len(loader)\n",
        "    avg_psnr = psnr_total / len(loader)\n",
        "    avg_ssim = ssim_total / len(loader)\n",
        "    return avg_loss, avg_psnr, avg_ssim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Checkpoint Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize best validation loss tracker\n",
        "best_val_loss = float('inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_checkpoint(epoch, model, optimizer, is_best=False):\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'best_metric': best_val_loss\n",
        "    }\n",
        "    \n",
        "    # Ensure save directory exists\n",
        "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "    \n",
        "    # Always save latest\n",
        "    latest_path = os.path.join(SAVE_DIR, \"latest.pth\")\n",
        "    torch.save(state, latest_path)\n",
        "    \n",
        "    # Save periodically\n",
        "    if epoch % SAVE_FREQ == 0:\n",
        "        periodic_path = os.path.join(SAVE_DIR, f\"epoch_{epoch}.pth\")\n",
        "        torch.save(state, periodic_path)\n",
        "    \n",
        "    # Save best separately\n",
        "    if is_best:\n",
        "        best_path = os.path.join(SAVE_DIR, \"best.pth\")\n",
        "        torch.save(state, best_path)\n",
        "        print(f\"New best model saved at epoch {epoch} with val loss: {best_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper to format time\n",
        "def format_time(seconds):\n",
        "    if seconds < 60:\n",
        "        return f\"{seconds:.0f}s\"\n",
        "    elif seconds < 3600:\n",
        "        return f\"{seconds//60:.0f}m {seconds%60:.0f}s\"\n",
        "    else:\n",
        "        return f\"{seconds//3600:.0f}h {(seconds%3600)//60:.0f}m\"\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "epoch_times = []\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Initialize main progress bar\n",
        "    pbar = tqdm(range(NUM_EPOCHS), desc=\"Training\", unit=\"epoch\")\n",
        "    \n",
        "    for epoch in pbar:\n",
        "        epoch_start = time.time()\n",
        "        \n",
        "        # --- Training Phase ---\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_psnr = 0.0\n",
        "        train_ssim = 0.0\n",
        "        \n",
        "        # Batch progress bar\n",
        "        batch_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", leave=False)\n",
        "        for batch_idx, (lr, hr) in enumerate(batch_pbar):\n",
        "            lr, hr = lr.to(device), hr.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(lr)\n",
        "            outputs = outputs[:, :, :hr.size(2), :hr.size(3)]\n",
        "            loss = compute_loss(outputs, hr)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Update metrics\n",
        "            psnr, ssim = compute_metrics(outputs, hr)\n",
        "            train_loss += loss.item()\n",
        "            train_psnr += psnr\n",
        "            train_ssim += ssim\n",
        "            \n",
        "            # Update batch progress\n",
        "            avg_loss = train_loss / (batch_idx + 1)\n",
        "            batch_pbar.set_postfix({\n",
        "                'loss': f\"{avg_loss:.4f}\",\n",
        "                'psnr': f\"{train_psnr/(batch_idx+1):.2f}\"\n",
        "            })\n",
        "            \n",
        "        batch_pbar.close()\n",
        "        \n",
        "        # --- Validation Phase ---\n",
        "        val_loss, val_psnr, val_ssim = validate(model, val_loader)\n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        # --- Epoch Timing ---\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        epoch_times.append(epoch_time)\n",
        "        avg_epoch_time = np.mean(epoch_times[-5:])  # Moving average of last 5 epochs\n",
        "        remaining_time = avg_epoch_time * (NUM_EPOCHS - epoch - 1)\n",
        "        \n",
        "        # Update main progress bar\n",
        "        pbar.set_postfix({\n",
        "            'val_loss': f\"{val_loss:.4f}\",\n",
        "            'val_psnr': f\"{val_psnr:.2f}\",\n",
        "            'eta': format_time(remaining_time)\n",
        "        })\n",
        "        \n",
        "        # --- Model Checkpointing ---\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            save_checkpoint(epoch, model, optimizer, is_best=True)\n",
        "        else:\n",
        "            save_checkpoint(epoch, model, optimizer)\n",
        "        \n",
        "        # --- Logging ---\n",
        "        wandb.log({\n",
        "            \"train/loss\": train_loss/len(train_loader),\n",
        "            \"train/psnr\": train_psnr/len(train_loader),\n",
        "            \"train/ssim\": train_ssim/len(train_loader),\n",
        "            \"val/loss\": val_loss,\n",
        "            \"val/psnr\": val_psnr,\n",
        "            \"val/ssim\": val_ssim,\n",
        "            \"epoch_time\": epoch_time,\n",
        "            \"lr\": optimizer.param_groups[0]['lr']\n",
        "        })\n",
        "        \n",
        "        # --- Visualizations ---\n",
        "        if (epoch + 1) % 2 == 0:  # Every 2 epochs to reduce overhead\n",
        "            log_predictions(model, epoch)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nTraining interrupted! Saving latest model...\")\n",
        "    save_checkpoint(epoch, model, optimizer)\n",
        "    print(f\"Safe to exit now. Total runtime: {format_time(time.time() - start_time)}\")\n",
        "    wandb.finish()\n",
        "finally:\n",
        "    pbar.close()\n",
        "\n",
        "print(f\"\\nTraining complete! Total duration: {format_time(time.time() - start_time)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing & Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_best_model():\n",
        "    checkpoint = torch.load(os.path.join(SAVE_DIR, \"best.pth\"))\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    return model\n",
        "\n",
        "# Load best model\n",
        "model = load_best_model().to(device)\n",
        "\n",
        "# Test evaluation\n",
        "test_loss, test_psnr, test_ssim = validate(model, test_loader)\n",
        "\n",
        "print(f\"\\nFinal Test Results:\")\n",
        "print(f\"Loss: {test_loss:.4f}\")\n",
        "print(f\"PSNR: {test_psnr:.2f} dB\")\n",
        "print(f\"SSIM: {test_ssim:.4f}\")\n",
        "\n",
        "# Log to WandB\n",
        "wandb.log({\n",
        "    \"test/loss\": test_loss,\n",
        "    \"test/psnr\": test_psnr,\n",
        "    \"test/ssim\": test_ssim\n",
        "})\n",
        "\n",
        "# Save sample visualizations\n",
        "def save_samples(loader, num_samples=3):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        samples = []\n",
        "        for lr, hr in loader:\n",
        "            lr = lr.to(device)\n",
        "            pred = model(lr)\n",
        "            \n",
        "            # Upscale LR to match HR dimensions\n",
        "            lr_upsampled = torch.nn.functional.interpolate(\n",
        "                lr[:, 3:6],  # Center frame\n",
        "                size=hr.shape[-2:],  # Target size (480x854)\n",
        "                mode='bicubic',\n",
        "                align_corners=False\n",
        "            )\n",
        "            \n",
        "            # Convert to CPU and denormalize\n",
        "            lr_vis = (lr_upsampled + 1) / 2\n",
        "            pred_vis = (pred + 1) / 2\n",
        "            hr_vis = (hr + 1) / 2\n",
        "\n",
        "            # Create comparison samples\n",
        "            for i in range(num_samples):\n",
        "                sample = torch.cat([\n",
        "                    lr_vis[i].cpu(),\n",
        "                    pred_vis[i].cpu(),\n",
        "                    hr_vis[i].cpu()\n",
        "                ], dim=-1)  # Concatenate along width\n",
        "                samples.append(sample)\n",
        "            \n",
        "            break  # Only first batch\n",
        "        \n",
        "        grid = torchvision.utils.make_grid(samples, nrow=1)\n",
        "        wandb.log({\"Test Results\": wandb.Image(grid, caption=\"LR | Pred | HR\")})\n",
        "\n",
        "save_samples(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualization & Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# # Loss\n",
        "# plt.subplot(1, 3, 1)\n",
        "# plt.plot(wandb.run.history()['train/loss'], label='Train')\n",
        "# plt.plot(wandb.run.history()['val/loss'], label='Validation')\n",
        "# plt.title('Loss Curve')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend()\n",
        "\n",
        "# # PSNR\n",
        "# plt.subplot(1, 3, 2)\n",
        "# plt.plot(wandb.run.history()['train/psnr'], label='Train')\n",
        "# plt.plot(wandb.run.history()['val/psnr'], label='Validation')\n",
        "# plt.title('PSNR')\n",
        "# plt.xlabel('Epoch')\n",
        "\n",
        "# # SSIM\n",
        "# plt.subplot(1, 3, 3)\n",
        "# plt.plot(wandb.run.history()['train/ssim'], label='Train')\n",
        "# plt.plot(wandb.run.history()['val/ssim'], label='Validation')\n",
        "# plt.title('SSIM')\n",
        "# plt.xlabel('Epoch')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Additional analysis\n",
        "wandb.finish()\n",
        "print(\"Training completed and results logged!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
